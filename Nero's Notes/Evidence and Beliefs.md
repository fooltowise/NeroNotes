How much evidence is needed to start trusting a belief? It all depends on how wide is the possibilities space in which the belief lies. 

For instance, if the tested hypothesis H has odds of 1:1'000 to occur, and we define "I trust H if its probability is higher than 0.5", then the evidence needs to be 1'000 times more likely to happen in a world where H is true than in a world in which H is false. Then the updated odds would be superior or equal to 1000:1000, and p(H) superior or equal to 0.5.

In information theory, we use bits to measure the strength of an evidence. The measure is taken in Log base 1/2 of the likelihood ratio. As 1'000 is roughly equal to 2^10, a likelihood ratio of 1'000 would bring 10 bits of evidence.
Similarly, initial odds of 1:1'000'000 for H to be true would need around at least 20 bits of evidence if we want the probability of H to be superior to 0.5. Having 24 bits of evidence would bring the posterior odds to 16:1, and 27 bits of evidence would lead to odds of 128:1, in other terms there would be slightly less than  a 1% chance than H is false.

To assign more than 50% probability to the correct candidate from a pool of 100,000,000 possible hypotheses, you need at least 27 bits of evidence (or thereabouts). You cannot expect to find the correct candidate without tests that are this strong, because lesser tests will yield more than one candidate that passes all the tests. If you try to apply a test that only has a million-to-one chance of a false positive (~ 20 bits), you’ll end up with a hundred candidates. Just *finding* the right answer, within a large space of possibilities, requires a large amount of evidence.

Traditional Rationality emphasizes justification: “If you want to convince me of X, you’ve got to present me with Y amount of evidence.” I myself often slip into this phrasing, whenever I say something like, “To *justify* believing in this proposition, at more than 99% probability, requires 34 bits of evidence.” 


See [[Beliefs]]